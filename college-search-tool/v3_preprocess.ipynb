{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkContext\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORECARD_PATH = \"./data/Most-Recent-Cohorts-All-Data-Elements.csv\"\n",
    "V2_PATH = \"./data/college-search-data-v2.parquet\"\n",
    "V3_PATH = \"./data/college-search-data-v3.parquet\"\n",
    "DATA_TYPES_PATH = \"./assets/datatypes.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datatypes(data_columns):\n",
    "\n",
    "    with open(DATA_TYPES_PATH) as f:\n",
    "        cur_datatypes = json.load(f)\n",
    "\n",
    "    for variable, datatype in data_columns.items():\n",
    "        cur_datatypes[variable] = datatype\n",
    "    \n",
    "    with open(DATA_TYPES_PATH, 'w') as f:\n",
    "        f.write(json.dumps(cur_datatypes, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6694, 551)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(V2_PATH)\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit current data types (Percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_columns = [\n",
    "    \"DVADM02\",\n",
    "    \"DVADM03\",\n",
    "    \"DVADM08\",\n",
    "    \"DVADM09\",\n",
    "    \"SATPCT\",\n",
    "    \"ACTPCT\",\n",
    "    \"DVADM04\",\n",
    "    \"RMOUSTTP\",\n",
    "    \"RMINSTTP\",\n",
    "    \"RMUNKNWP\",\n",
    "    \"BAGR100\",\n",
    "    \"BAGR150\",\n",
    "    \"ANYAIDP\",\n",
    "    \"PGRNT_P\",\n",
    "    \"OFGRT_P\",\n",
    "    \"FLOAN_P\",\n",
    "    \"OLOAN_P\",\n",
    "    \"UAGRNTP\",\n",
    "    \"UPGRNTP\",\n",
    "    \"UFLOANP\",\n",
    "    \"PCTENRBK\",\n",
    "    \"PCTENRHS\",\n",
    "    \"PCTENRAP\",\n",
    "    \"PCTENRAN\",\n",
    "    \"PCTENRNR\",\n",
    "    \"PCTENRUN\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_TYPES_PATH) as f:\n",
    "    cur_datatypes = json.load(f)\n",
    "\n",
    "for per_col in percent_columns:\n",
    "    df = df.withColumn(per_col, F.col(per_col).cast(\"float\"))\n",
    "    df = df.withColumn(per_col, col(per_col) / 100)\n",
    "    cur_datatypes[per_col] = \"float\"\n",
    "\n",
    "with open(DATA_TYPES_PATH, 'w') as f:\n",
    "    f.write(json.dumps(cur_datatypes, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_columns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Override Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new columns for override\n",
    "hd2020_df = spark.read.csv(\"./data/v2_additional_data/HD2020.csv\", header=True, inferSchema=True)\n",
    "drvef2020_df = spark.read.csv(\"./data/v2_additional_data/DRVEF2020.csv\", header=True, inferSchema=True)\n",
    "\n",
    "override_nonexist_columns = {\n",
    "    \"WEBADDR\": \"string\",\n",
    "    \"NPRICURL\": \"string\",\n",
    "    \"PCTENR2M\": \"float\"\n",
    "}\n",
    "\n",
    "add_datatypes(override_nonexist_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "override_hd = hd2020_df.select(\"UNITID\", \"WEBADDR\", \"NPRICURL\")\n",
    "override_hd = override_hd.withColumn(\"UNITID\", F.col(\"UNITID\").cast(\"string\"))\n",
    "override_drvef = drvef2020_df.select(\"UNITID\", \"PCTENR2M\")\n",
    "override_drvef = override_drvef.withColumn(\"UNITID\", F.col(\"UNITID\").cast(\"string\"))\n",
    "\n",
    "df = df.join(override_hd, \"UNITID\", \"left\")\n",
    "df = df.join(override_drvef, \"UNITID\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"PCTENR2M\", F.col(\"PCTENR2M\").cast(\"float\"))\n",
    "df = df.withColumn(\"PCTENR2M\", col(\"PCTENR2M\") / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({'NULL': None, 'null': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Save dictionary of null value counts for reference\n",
    "with open('./assets/null_counts.json', 'w') as f:\n",
    "    json.dump(null_counts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_override(first, second):\n",
    "    if first is None:\n",
    "        return second\n",
    "    return first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_1 = udf(column_override, StringType())\n",
    "udf_2 = udf(column_override, FloatType())\n",
    "df = df.withColumn(\"INSTURL\", udf_1(\"WEBADDR\", \"INSTURL\"))\n",
    "df = df.withColumn(\"NPCURL\", udf_1(\"NPRICURL\", \"NPCURL\"))\n",
    "df = df.withColumn(\"UGDS_2MOR\", udf_2(\"PCTENR2M\", \"UGDS_2MOR\"))\n",
    "df = df.withColumn(\"UGDS_WHITE\", udf_2(\"PCTENRWH\", \"UGDS_WHITE\"))\n",
    "df = df.withColumn(\"UGDS_BLACK\", udf_2(\"PCTENRBK\", \"UGDS_BLACK\"))\n",
    "df = df.withColumn(\"UGDS_AIAN\", udf_2(\"PCTENRAN\", \"UGDS_AIAN\"))\n",
    "df = df.withColumn(\"UGDS_NRA\", udf_2(\"PCTENRNR\", \"UGDS_NRA\"))\n",
    "df = df.withColumn(\"UGDS_UNKN\", udf_2(\"PCTENRUN\", \"UGDS_UNKN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether override is success\n",
    "null_counts_2 = df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Save dictionary of null value counts for reference\n",
    "with open('./assets/null_counts_v3.json', 'w') as f:\n",
    "    json.dump(null_counts_2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PySpark to check and convert data types\n",
    "with open('./assets/datatypes.json') as f:\n",
    "    datatypes = json.load(f)\n",
    "\n",
    "# cast columns to correct datatypes\n",
    "for field, datatype in datatypes.items(): # takes a few mins to run\n",
    "    if field not in df.columns:\n",
    "        continue\n",
    "    curr_type = dict(df.dtypes)[field]\n",
    "    if curr_type != datatype and not datatype.startswith(curr_type):\n",
    "        df = df.withColumn(field, F.col(field).cast(datatype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save(V3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-override Variables (Already Included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_override_columns = {\n",
    "    \"CIP01BACHL\": \"integer\",\n",
    "    \"CIP03BACHL\": \"integer\",\n",
    "    \"CIP04BACHL\": \"integer\",\n",
    "    \"CIP05BACHL\": \"integer\",\n",
    "    \"CIP09BACHL\": \"integer\",\n",
    "    \"CIP10BACHL\": \"integer\",\n",
    "    \"CIP11BACHL\": \"integer\",\n",
    "    \"CIP12BACHL\": \"integer\",\n",
    "    \"CIP13BACHL\": \"integer\",\n",
    "    \"CIP14BACHL\": \"integer\",\n",
    "    \"CIP15BACHL\": \"integer\",\n",
    "    \"CIP16BACHL\": \"integer\",\n",
    "    \"CIP19BACHL\": \"integer\",\n",
    "    \"CIP22BACHL\": \"integer\",\n",
    "    \"CIP23BACHL\": \"integer\",\n",
    "    \"CIP24BACHL\": \"integer\",\n",
    "    \"CIP25BACHL\": \"integer\",\n",
    "    \"CIP26BACHL\": \"integer\",\n",
    "    \"CIP27BACHL\": \"integer\",\n",
    "    \"CIP29BACHL\": \"integer\",\n",
    "    \"CIP30BACHL\": \"integer\",\n",
    "    \"CIP31BACHL\": \"integer\",\n",
    "    \"CIP38BACHL\": \"integer\",\n",
    "    \"CIP39BACHL\": \"integer\",\n",
    "    \"CIP40BACHL\": \"integer\",\n",
    "    \"CIP41BACHL\": \"integer\",\n",
    "    \"CIP42BACHL\": \"integer\",\n",
    "    \"CIP43BACHL\": \"integer\",\n",
    "    \"CIP44BACHL\": \"integer\",\n",
    "    \"CIP45BACHL\": \"integer\",\n",
    "    \"CIP46BACHL\": \"integer\",\n",
    "    \"CIP47BACHL\": \"integer\",\n",
    "    \"CIP48BACHL\": \"integer\",\n",
    "    \"CIP49BACHL\": \"integer\",\n",
    "    \"CIP50BACHL\": \"integer\",\n",
    "    \"CIP51BACHL\": \"integer\",\n",
    "    \"CIP52BACHL\": \"integer\",\n",
    "    \"CIP54BACHL\": \"integer\",\n",
    "    \"NPT41_PUB\": \"integer\",\n",
    "    \"NPT42_PUB\": \"integer\",\n",
    "    \"NPT43_PUB\": \"integer\",\n",
    "    \"NPT44_PUB\": \"integer\",\n",
    "    \"NPT45_PUB\": \"integer\",\n",
    "    \"NPT41_PRIV\": \"integer\",\n",
    "    \"NPT42_PRIV\": \"integer\",\n",
    "    \"NPT43_PRIV\": \"integer\",\n",
    "    \"NPT44_PRIV\": \"integer\",\n",
    "    \"NPT45_PRIV\": \"integer\",\n",
    "    \"GRAD_DEBT_MDN\": \"float\",\n",
    "    \"LO_INC_DEBT_MDN\": \"float\",\n",
    "    \"MD_INC_DEBT_MDN\": \"float\",\n",
    "    \"HI_INC_DEBT_MDN\": \"float\",\n",
    "    \"FIRSTGEN_DEBT_MDN\": \"float\",\n",
    "    \"NOTFIRSTGEN_DEBT_MDN\": \"float\",\n",
    "    \"FIRST_GEN\": \"float\",\n",
    "    \"UNEMP_RATE\": \"float\",\n",
    "    \"MN_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT10_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT25_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT75_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT90_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT10_EARN_WNE_P6\": \"integer\",\n",
    "    \"PCT25_EARN_WNE_P6\": \"integer\",\n",
    "    \"PCT75_EARN_WNE_P6\": \"integer\",\n",
    "    \"PCT90_EARN_WNE_P6\": \"integer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add datatypes\n",
    "add_datatypes(non_override_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6694, 2989)\n"
     ]
    }
   ],
   "source": [
    "scorecard_df = spark.read.csv(SCORECARD_PATH, header=True, inferSchema=True)\n",
    "print((scorecard_df.count(), len(scorecard_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_override_colnames = [\"UNITID\"]\n",
    "for colname, datatype in non_override_columns.items():\n",
    "    non_override_colnames.append(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_override_df = scorecard_df.select(*non_override_colnames)\n",
    "non_override_df = non_override_df.withColumn(\"UNITID\", F.col(\"UNITID\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6694, 66)\n"
     ]
    }
   ],
   "source": [
    "print((non_override_df.count(), len(non_override_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6694, 616)\n"
     ]
    }
   ],
   "source": [
    "df = df.join(non_override_df, \"UNITID\", \"left\")\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551\n",
      "616\n"
     ]
    }
   ],
   "source": [
    "col_set = set(df.columns)\n",
    "print(len(col_set))\n",
    "print(len(df.columns))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a17e3a3172cf46aaf637e4c7681713daf53374be115d411b165dfc3d728fe3c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
