{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified by Rithwick (for debugging)\n",
    "def stringifyTuple(tup):\n",
    "    str1 = \"{\" + ','.join(map(str,tup)) + \"}\"\n",
    "    return str1\n",
    "\n",
    "# Initialize SparkContext\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 1: data_preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Update necessary fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"./data/Most-Recent-Cohorts-All-Data-Elements.csv\", low_memory=False)\n",
    "shape = data.shape\n",
    "# Modified by Rithwick (for debugging)\n",
    "print(\"Shape of data: \" + stringifyTuple(shape))\n",
    "data_dict = pd.read_csv(\"./data/Institution_data_dictionary.csv\")\n",
    "\n",
    "shape = data_dict.shape\n",
    "print(\"Shape of data_dict: \" + stringifyTuple(shape))\n",
    "\n",
    "# read previous fields json file\n",
    "with open(\"./assets/fields.json\") as prevFields:\n",
    "   prevfields_json = json.load(prevFields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "data_dict_name_map = data_dict[['VARIABLE NAME', 'developer-friendly name']]\n",
    "\n",
    "# re-insert all variable-name & developer-friendly name pairs\n",
    "for variable_name in prevfields_json.keys():\n",
    "   prevfields_json[variable_name] = data_dict_name_map.loc[data_dict_name_map[\"VARIABLE NAME\"] == variable_name, \"developer-friendly name\"].values[0]\n",
    "\n",
    "# write the updated fields into fields.json\n",
    "# Modified by Rithwick (the fields.json file is not there on the GitHub project)  \n",
    "# Copied fields-v1.json from the GitHub project directory\n",
    "with open(\"./assets/fields.json\", \"w\") as curFields:\n",
    "   curFields.write(json.dumps(prevfields_json, indent=4))\n",
    "\n",
    "# filter dataset with only fields in the fields.json\n",
    "with open(\"./assets/fields.json\") as curFields:\n",
    "   dataFields = json.load(curFields)\n",
    "   data = data[list(dataFields.keys())]\n",
    "shape = data.shape\n",
    "# Modified by Rithwick (for debugging)\n",
    "print(\"Shape of data: \" + stringifyTuple(shape))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Update and Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read previous datatypes.json file\n",
    "with open(\"./assets/datatypes.json\") as prevDTypes:\n",
    "   prevDTypes_json = json.load(prevDTypes)\n",
    "\n",
    "# Select API data type and VARIABLE NAME from data_dict\n",
    "data_dict_data_type = data_dict[['VARIABLE NAME', 'API data type']]\n",
    "# Modified by Rithwick (for debugging)\n",
    "print (data_dict_data_type)\n",
    "\n",
    "for dtype_name in prevDTypes_json.keys():\n",
    "   # Modified by Rithwick (for handling missing data types in \"Institution_data_dictionary.csv\")\n",
    "   try:\n",
    "       prevDTypes_json[dtype_name] = data_dict_data_type.loc[data_dict_data_type[\"VARIABLE NAME\"] == dtype_name, \"API data type\"].values[0]\n",
    "   except IndexError as ex:\n",
    "       print(\"Ignoring Error {0}: dtype_name = (\".format(str(ex)) + dtype_name + \")\")\n",
    "\n",
    "# write the updated fields into datatypes.json\n",
    "# Modified by Rithwick (for avoid touching original file)\n",
    "with open(\"./assets/datatypes-updated.json\", \"w\") as curDTypes:\n",
    "   prevDTypes_json[\"INSTNM\"] = \"string\"\n",
    "   prevDTypes_json[\"CITY\"] = \"string\"\n",
    "   prevDTypes_json[\"UNITID\"] = \"string\"\n",
    "   curDTypes.write(json.dumps(prevDTypes_json, indent=4))\n",
    "\n",
    "# save current data into csv\n",
    "data.to_csv(\"./data/Necessary-Fields-Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset using PySpark\n",
    "df = spark.read.csv(\"./data/Necessary-Fields-Data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# use PySpark to check and convert data types\n",
    "# Modified by Rithwick (for reading from modified file)\n",
    "with open('./assets/datatypes.json') as f:\n",
    "   datatypes = json.load(f)\n",
    "\n",
    "# cast columns to correct datatypes\n",
    "for field, datatype in datatypes.items(): # takes a few mins to run\n",
    "    if field not in df.columns:\n",
    "       continue\n",
    "    curr_type = dict(df.dtypes)[field]\n",
    "    if curr_type != datatype and not datatype.startswith(curr_type):\n",
    "       df = df.withColumn(field, F.col(field).cast(datatype))\n",
    "print('Modified CSV (with correct datatypes): # of columns = ', len(df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Filter Empty (Null) Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all \\\"NULL\\\" values in dataframe with literal null values so that isnull() can be used\n",
    "df = df.replace({'NULL': None, 'null': None})\n",
    "\n",
    "null_counts = df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Drop fields that are completely null\n",
    "num_rows = df.count()\n",
    "for field, null_count in null_counts.items():\n",
    "    if null_count >= num_rows: # all values of field are null, drop the field\n",
    "        df = df.drop(field)\n",
    "print('Modified CSV (with NULL columns removed): # of columns = ', len(df.columns)) # number of fields that are not completely null\n",
    "\n",
    "# Save dictionary of null value counts for reference\n",
    "with open('./assets/null_counts.json', 'w') as f:\n",
    "    json.dump(null_counts, f)\n",
    "\n",
    "# Test that casting worked: Howard University should be true for HBCU\n",
    "df.where(df.INSTNM.contains('Howard')).select([\"INSTNM\", \"HBCU\"]).show(truncate=False)\n",
    "\n",
    "# replace \"CCBASIC\" with any field name (or add other field names to the select list) from fields.json to print out specific fields for UW campuses\n",
    "df.where(df.INSTNM.contains('University of Washington')).select([\"INSTNM\", \"CCBASIC\"]).show(truncate=False)\n",
    "\n",
    "# drop \"_c0\" column\n",
    "df = df.drop(\"_c0\")\n",
    "\n",
    "# save the dataframe\n",
    "# df_filename = './data/college-search-data.parquet'\n",
    "# df.write.mode('overwrite').save(df_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 2: v2_additionalData_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes_route = \"./assets/datatypes.json\"\n",
    "v2_additional_route = \"./data/v2_additional_data/\"\n",
    "\n",
    "# change ic2020mission.csv column name into uppercase\n",
    "ic2020mission_df = pd.read_csv(\"./data/v2_additional_data/IC2020Mission.csv\")\n",
    "ic2020mission_df = ic2020mission_df.rename(columns={\"unitid\":\"UNITID\", \"missionURL\": \"MISSIONURL\", \"mission\": \"MISSION\"})\n",
    "\n",
    "shape = ic2020mission_df.shape\n",
    "# Modified by Rithwick (for debugging)\n",
    "print(\"Shape of data(ic2020mission_df): \" + stringifyTuple(shape))\n",
    "\n",
    "ic2020mission_df.to_csv(\"./data/v2_additional_data/IC2020Mission_updated.csv\")\n",
    "\n",
    "# EF2020A only selecting EFALEVEL == 2\n",
    "ef2020a = pd.read_csv(v2_additional_route + \"EF2020A.csv\")\n",
    "ef2020a_update = ef2020a[ef2020a[\"EFALEVEL\"] == 2]\n",
    "\n",
    "shape = ef2020a_update.shape\n",
    "# Modified by Rithwick (for debugging)\n",
    "print(\"Shape of data(ef2020a_update): \" + stringifyTuple(shape))\n",
    "\n",
    "ef2020a_update.to_csv(v2_additional_route + \"EF2020A_updated.csv\")\n",
    "\n",
    "# EF2019B only selecting EFBAGE == 1, LSTUDY == 2\n",
    "ef2019b = pd.read_csv(v2_additional_route + \"EF2019B.csv\")\n",
    "ef2019b_updated = ef2019b[(ef2019b[\"EFBAGE\"] == 1) & (ef2019b[\"LSTUDY\"] == 2)]\n",
    "\n",
    "shape = ef2019b_updated.shape\n",
    "# Modified by Rithwick (for debugging)\n",
    "print(\"Shape of data (ef2019b_updated): \" + stringifyTuple(shape))\n",
    "\n",
    "ef2019b_updated.to_csv(v2_additional_route + \"EF2019B_updated.csv\")\n",
    "\n",
    "# SAL2020_IS only selecting ARANK == 7\n",
    "sal2020_is = pd.read_csv(v2_additional_route + \"SAL2020_IS.csv\")\n",
    "sal2020_is_updated = sal2020_is[sal2020_is[\"ARANK\"] == 7]\n",
    "\n",
    "shape = sal2020_is_updated.shape\n",
    "# Modified by Rithwick (for debugging)\n",
    "print(\"Shape of data (sal2020_is_updated): \" + stringifyTuple(shape))\n",
    "\n",
    "sal2020_is_updated.to_csv(v2_additional_route + \"SAL2020_IS_updated.csv\")\n",
    "\n",
    "# GR2020_PELL_SSL only selecting PSGRTYPE == 1\n",
    "gr2020_pell_ssl = pd.read_csv(v2_additional_route + \"GR2020_PELL_SSL.csv\")\n",
    "gr2020_pell_ssl_updated = gr2020_pell_ssl[gr2020_pell_ssl[\"PSGRTYPE\"] == 1]\n",
    "\n",
    "shape = gr2020_pell_ssl_updated.shape\n",
    "# Modified by Rithwick (for debugging)\n",
    "print(\"Shape of data (gr2020_pell_ssl_updated): \" + stringifyTuple(shape))\n",
    "\n",
    "gr2020_pell_ssl_updated.to_csv(v2_additional_route + \"GR2020_PELL_SSL_updated.csv\")\n",
    "\n",
    "# Initialize SparkContext\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "#sc = spark.sparkContext\n",
    "\n",
    "# import dataset\n",
    "#df_filename = './data/college-search-data.parquet'\n",
    "#df = spark.read.load(df_filename)\n",
    "\n",
    "#print((df.count(), len(df.columns)))\n",
    "\n",
    "# 10 columns\n",
    "hd2020_columns = {\n",
    "    \"ADDR\": \"string\",\n",
    "    \"GENTELE\": \"string\",\n",
    "    \"INSTCAT\": \"integer\",\n",
    "    \"LANDGRNT\": \"integer\",\n",
    "    \"C18IPUG\": \"integer\",\n",
    "    \"C18UGPRF\": \"integer\",\n",
    "    \"C18ENPRF\": \"integer\",\n",
    "    \"C18SZSET\": \"integer\",\n",
    "    \"IALIAS\": \"string\",\n",
    "    \"INSTSIZE\": \"integer\"\n",
    "}\n",
    "\n",
    "# 12 columns - 2 columns\n",
    "ic2020_columns = {\n",
    "    # \"RELAFFIL\": \"integer\",\n",
    "    # \"OPENADMP\": \"integer\",\n",
    "    \"SLO5\": \"integer\",\n",
    "    \"ASSOC1\": \"integer\",\n",
    "    \"SPORT1\": \"integer\",\n",
    "    \"SPORT2\": \"integer\",\n",
    "    \"SPORT3\": \"integer\",\n",
    "    \"SPORT4\": \"integer\",\n",
    "    \"CALSYS\": \"integer\",\n",
    "    \"APPLFEEU\": \"integer\",\n",
    "    \"FT_UG\": \"integer\",\n",
    "    \"RMBRDAMT\": \"integer\"\n",
    "}\n",
    "\n",
    "# 11 columns\n",
    "adm2020_columns = {\n",
    "    \"ADMCON1\": \"integer\",\n",
    "    \"ADMCON2\": \"integer\",\n",
    "    \"ADMCON3\": \"integer\",\n",
    "    \"ADMCON4\": \"integer\",\n",
    "    \"ADMCON5\": \"integer\",\n",
    "    \"ADMCON6\": \"integer\",\n",
    "    \"ADMCON7\": \"integer\",\n",
    "    \"SATPCT\": \"integer\",\n",
    "    \"ACTPCT\": \"integer\",\n",
    "    \"ENRLM\": \"integer\",\n",
    "    \"ENRLW\": \"integer\"\n",
    "}\n",
    "\n",
    "# 5 columns\n",
    "drvadm2020_columns = {\n",
    "    \"DVADM02\": \"integer\",\n",
    "    \"DVADM03\": \"integer\",\n",
    "    \"DVADM08\": \"integer\",\n",
    "    \"DVADM09\": \"integer\",\n",
    "    \"DVADM04\": \"integer\"\n",
    "}\n",
    "\n",
    "# 1 column\n",
    "ic2020mission_columns = {\n",
    "    \"MISSION\": \"string\"\n",
    "}\n",
    "\n",
    "# 5 columns\n",
    "drvic2020_columns = {\n",
    "    \"CINSON\": \"integer\",\n",
    "    \"COTSON\": \"integer\",\n",
    "    \"CINSOFF\": \"integer\",\n",
    "    \"COTSOFF\": \"integer\",\n",
    "    \"TUFEYR3\": \"integer\",\n",
    "}\n",
    "\n",
    "# 2 columns\n",
    "ic2020_ay_columns = {\n",
    "    \"TUITION2\": \"integer\",\n",
    "    \"TUITION3\": \"integer\"\n",
    "}\n",
    "\n",
    "# 1 column\n",
    "ef2020a_columns = {\n",
    "    \"EFNRALT\": \"integer\"\n",
    "}\n",
    "\n",
    "# 2 columns\n",
    "ef2020b_columns = {\n",
    "    \"EFAGE07\": \"integer\",\n",
    "    \"EFAGE08\": \"integer\"\n",
    "}\n",
    "\n",
    "# 13 columns\n",
    "drvef2020_columns = {\n",
    "    \"ENRTOT\": \"integer\",\n",
    "    \"EFUG\": \"integer\",\n",
    "    \"EFGRAD\": \"integer\",\n",
    "    \"RMOUSTTP\": \"integer\",\n",
    "    \"RMINSTTP\": \"integer\",\n",
    "    \"RMUNKNWP\": \"integer\",\n",
    "    \"PCTENRWH\": \"integer\",\n",
    "    \"PCTENRBK\": \"integer\",\n",
    "    \"PCTENRHS\": \"integer\",\n",
    "    \"PCTENRAP\": \"integer\",\n",
    "    \"PCTENRAN\": \"integer\",\n",
    "    \"PCTENRUN\": \"integer\",\n",
    "    \"PCTENRNR\": \"integer\",\n",
    "}\n",
    "\n",
    "# 1 column\n",
    "ef2020d_columns = {\n",
    "    \"STUFACR\": \"integer\"\n",
    "}\n",
    "\n",
    "# new dataset:\n",
    "# 3 columns\n",
    "sal2020_is_columns = {\n",
    "    \"SAINSTT\": \"integer\",\n",
    "    \"SAINSTW\": \"integer\",\n",
    "    \"SAINSTM\": \"integer\",\n",
    "}\n",
    "\n",
    "# 1 column\n",
    "sal2020_nis_columns = {\n",
    "    \"SANIN02\": \"integer\",\n",
    "}\n",
    "\n",
    "# 6 columns\n",
    "f1920_f2_columns = {\n",
    "    \"F2C01\": \"integer\",\n",
    "    \"F2C02\": \"integer\",\n",
    "    \"F2C03\": \"integer\",\n",
    "    \"F2C04\": \"integer\",\n",
    "    \"F2C07\": \"integer\",\n",
    "    \"F2E081\": \"integer\",\n",
    "}\n",
    "\n",
    "# 3 columns\n",
    "drvf2020_columns = {\n",
    "    \"F1STSVFT\": \"integer\",\n",
    "    \"F1ACSPFT\": \"integer\",\n",
    "    \"F1OTEXFT\": \"integer\",\n",
    "}\n",
    "\n",
    "# 15 columns\n",
    "sfa1920_p2_columns = {\n",
    "    \"GIS4OF1\": \"integer\",\n",
    "    \"GIS4A41\": \"integer\",\n",
    "    \"GIS4T51\": \"integer\",\n",
    "    \"NPT430\": \"integer\",\n",
    "    \"NPT440\": \"integer\",\n",
    "    \"NPT450\": \"integer\",\n",
    "    \"GRN4G11\": \"integer\",\n",
    "    \"GRN4G21\": \"integer\",\n",
    "    \"GRN4G31\": \"integer\",\n",
    "    \"GIS4A12\": \"integer\",\n",
    "    \"GIS4A22\": \"integer\",\n",
    "    \"GIS4A32\": \"integer\",\n",
    "    \"GIS4A42\": \"integer\",\n",
    "    \"GIS4A52\": \"integer\",\n",
    "    \"NPIST2\": \"integer\",\n",
    "}\n",
    "\n",
    "# 2 columns\n",
    "gr200_20_columns = {\n",
    "    \"BAGR100\": \"integer\",\n",
    "    \"BAGR150\": \"integer\",\n",
    "}\n",
    "\n",
    "# 2 columns\n",
    "gr2020_pell_ssl_columns = {\n",
    "    \"NRCMOBA\": \"integer\",\n",
    "    \"NRCMTOT\": \"integer\",\n",
    "}\n",
    "\n",
    "# 10 columns\n",
    "sfa1920_p1_columns = {\n",
    "    \"SCFA2\": \"integer\",\n",
    "    \"ANYAIDP\": \"integer\",\n",
    "    \"PGRNT_P\": \"integer\",\n",
    "    \"OFGRT_P\": \"integer\",\n",
    "    \"FLOAN_P\": \"integer\",\n",
    "    \"OLOAN_P\": \"integer\",\n",
    "    \"UAGRNTP\": \"integer\",\n",
    "    \"UPGRNTP\": \"integer\",\n",
    "    \"UFLOANP\": \"integer\",\n",
    "    \"AGRNT_A\": \"integer\",\n",
    "}\n",
    "\n",
    "# 105 - 2 columns overall\n",
    "\n",
    "def concatenate_new_data(datatype_dict, dataset_name, df):\n",
    "    colNames = list(datatype_dict.keys())\n",
    "    with open(datatypes_route) as f:\n",
    "        cur_datatypes = json.load(f)\n",
    "\n",
    "    # print(len(cur_datatypes))\n",
    "\n",
    "    for colName in colNames:\n",
    "        cur_datatypes[colName] = datatype_dict[colName]\n",
    "\n",
    "    # print(len(cur_datatypes))\n",
    "\n",
    "    with open(datatypes_route, 'w') as f:\n",
    "        f.write(json.dumps(cur_datatypes, indent=4))\n",
    "\n",
    "    colNames.append(\"UNITID\")\n",
    "\n",
    "    v2df = spark.read.csv(v2_additional_route + dataset_name, header=True, inferSchema=True)\n",
    "    v2df = v2df.withColumn(\"UNITID\", F.col(\"UNITID\").cast(\"string\"))\n",
    "    v2df = v2df.select(colNames)\n",
    "\n",
    "    df = df.join(v2df, \"UNITID\", \"left\")\n",
    "    print(\"Dataset name = {0}, count = {1}, columns = {2}\". format(dataset_name, df.count(), len(df.columns)))\n",
    "    #print((df.count(), len(df.columns)))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Initialize SparkContext\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# import dataset\n",
    "df_filename = './data/college-search-data.parquet'\n",
    "df = spark.read.load(df_filename)\n",
    "\n",
    "print(\"Parquet File details: name = {0}, count = {1}, columns = {2}\". format(df_filename, df.count(), len(df.columns)))\n",
    "#print(\"college-search-data.parquet (Count, Columns) = \", (df.count(), len(df.columns)))\n",
    "\n",
    "# Start merging the first run parquet file with additional data  \n",
    "df = concatenate_new_data(hd2020_columns, \"HD2020.csv\", df)\n",
    "df = concatenate_new_data(ic2020_columns, \"IC2020.csv\", df)\n",
    "df = concatenate_new_data(adm2020_columns, \"ADM2020.csv\", df)\n",
    "df = concatenate_new_data(drvadm2020_columns, \"DRVADM2020.csv\", df)\n",
    "df = concatenate_new_data(ic2020mission_columns, \"IC2020Mission_updated.csv\", df)\n",
    "df = concatenate_new_data(drvic2020_columns, \"DRVIC2020.csv\", df)\n",
    "df = concatenate_new_data(ic2020_ay_columns, \"IC2020_AY.csv\", df)\n",
    "df = concatenate_new_data(ef2020a_columns, \"EF2020A_updated.csv\", df)\n",
    "df = concatenate_new_data(ef2020b_columns, \"EF2019B_updated.csv\", df)\n",
    "df = concatenate_new_data(drvef2020_columns, \"DRVEF2020.csv\", df)\n",
    "df = concatenate_new_data(ef2020d_columns, \"EF2020D.csv\", df)\n",
    "df = concatenate_new_data(sal2020_is_columns, \"SAL2020_IS_updated.csv\", df)\n",
    "df = concatenate_new_data(sal2020_nis_columns, \"SAL2020_NIS.csv\", df)\n",
    "df = concatenate_new_data(f1920_f2_columns, \"F1920_F2.csv\", df)\n",
    "df = concatenate_new_data(drvf2020_columns, \"DRVF2020.csv\", df)\n",
    "df = concatenate_new_data(sfa1920_p2_columns, \"SFA1920_P2.csv\", df)\n",
    "df = concatenate_new_data(gr200_20_columns, \"GR200_20.csv\", df)\n",
    "df = concatenate_new_data(gr2020_pell_ssl_columns, \"GR2020_PELL_SSL_updated.csv\", df)\n",
    "df = concatenate_new_data(sfa1920_p1_columns, \"SFA1920_P1.csv\", df)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def target_tier(adm_rate):\n",
    "    #print(adm_rate)\n",
    "    if adm_rate == None:\n",
    "        return None\n",
    "    if adm_rate < 0.1:\n",
    "        return 13\n",
    "    elif adm_rate < 0.14:\n",
    "        return 10\n",
    "    elif adm_rate < 0.17:\n",
    "        return 9\n",
    "    elif adm_rate < 0.21:\n",
    "        return 8\n",
    "    elif adm_rate < 0.26:\n",
    "        return 7\n",
    "    elif adm_rate < 0.33:\n",
    "        return 6\n",
    "    elif adm_rate < 0.42:\n",
    "        return 5\n",
    "    elif adm_rate < 0.55:\n",
    "        return 4\n",
    "    elif adm_rate < 0.75:\n",
    "        return 3\n",
    "    elif adm_rate < 0.85:\n",
    "        return 2\n",
    "    elif adm_rate < 0.90:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def safety_tier(adm_rate):\n",
    "    if adm_rate == None:\n",
    "        return None\n",
    "    if adm_rate < 0.1:\n",
    "        return 13\n",
    "    elif adm_rate < 0.14:\n",
    "        return 11\n",
    "    elif adm_rate < 0.18:\n",
    "        return 10\n",
    "    elif adm_rate < 0.23:\n",
    "        return 9\n",
    "    elif adm_rate < 0.31:\n",
    "        return 8\n",
    "    elif adm_rate < 0.45:\n",
    "        return 7\n",
    "    elif adm_rate < 0.6:\n",
    "        return 6\n",
    "    elif adm_rate < 0.75:\n",
    "        return 5\n",
    "    elif adm_rate < 0.9:\n",
    "        return 4\n",
    "    elif adm_rate < 0.95:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df = df.replace({'NULL': None, 'null': None})\n",
    "udf_target_tier = udf(target_tier, IntegerType())\n",
    "df_metric_1 = df.withColumn(\"TARGET_TIER\", udf_target_tier(\"ADM_RATE\"))\n",
    "udf_safety_tier = udf(safety_tier, IntegerType())\n",
    "df_metric_2 = df_metric_1.withColumn(\"SAFETY_TIER\", udf_safety_tier(\"ADM_RATE\"))\n",
    "print(\"Dataset (after applying target/safety): count = {0}, columns = {1}\". format(df_metric_2.count(), len(df_metric_2.columns)))\n",
    "#print((df_metric_2.count(), len(df_metric_2.columns)))\n",
    "\n",
    "with open(datatypes_route) as f:\n",
    "    curTypes = json.load(f)\n",
    "curTypes[\"TARGET_TIER\"] = \"integer\"\n",
    "curTypes[\"SAFETY_TIER\"] = \"integer\"\n",
    "\n",
    "with open(datatypes_route, 'w') as f:\n",
    "    f.write(json.dumps(curTypes, indent=4))\n",
    "\n",
    "\n",
    "# use PySpark to check and convert data types\n",
    "with open('./assets/datatypes.json') as f:\n",
    "    datatypes = json.load(f)\n",
    "\n",
    "# cast columns to correct datatypes\n",
    "for field, datatype in datatypes.items(): # takes a few mins to run\n",
    "    if field not in df_metric_2.columns:\n",
    "        continue\n",
    "    curr_type = dict(df_metric_2.dtypes)[field]\n",
    "    if curr_type != datatype and not datatype.startswith(curr_type):\n",
    "        df_metric_2 = df_metric_2.withColumn(field, F.col(field).cast(datatype))\n",
    "\n",
    "# Modified by Rithwick: Add \"overwrite\" mode to make the program run without removing the V2 Parquet directory\n",
    "# df_metric_2.write.mode('overwrite').save('./data/college-search-data-v2.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File 3: v3_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORECARD_PATH = \"./data/Most-Recent-Cohorts-All-Data-Elements.csv\"\n",
    "V2_PATH = \"./data/college-search-data-v2.parquet\"\n",
    "V3_PATH = \"./data/college-search-data-v3.parquet\"\n",
    "DATA_TYPES_PATH = \"./assets/datatypes.json\"\n",
    "\n",
    "def add_datatypes(data_columns):\n",
    "\n",
    "    with open(DATA_TYPES_PATH) as f:\n",
    "        cur_datatypes = json.load(f)\n",
    "\n",
    "    for variable, datatype in data_columns.items():\n",
    "        cur_datatypes[variable] = datatype\n",
    "    \n",
    "    with open(DATA_TYPES_PATH, 'w') as f:\n",
    "        f.write(json.dumps(cur_datatypes, indent=4))\n",
    "\n",
    "df = spark.read.load(V2_PATH)\n",
    "print(\"Read {0} (Count={1}, Columns={2})\".format(V2_PATH, df.count(), len(df.columns)))\n",
    "\n",
    "\n",
    "percent_columns = [\n",
    "    \"DVADM02\",\n",
    "    \"DVADM03\",\n",
    "    \"DVADM08\",\n",
    "    \"DVADM09\",\n",
    "    \"SATPCT\",\n",
    "    \"ACTPCT\",\n",
    "    \"DVADM04\",\n",
    "    \"RMOUSTTP\",\n",
    "    \"RMINSTTP\",\n",
    "    \"RMUNKNWP\",\n",
    "    \"BAGR100\",\n",
    "    \"BAGR150\",\n",
    "    \"ANYAIDP\",\n",
    "    \"PGRNT_P\",\n",
    "    \"OFGRT_P\",\n",
    "    \"FLOAN_P\",\n",
    "    \"OLOAN_P\",\n",
    "    \"UAGRNTP\",\n",
    "    \"UPGRNTP\",\n",
    "    \"UFLOANP\",\n",
    "    \"PCTENRBK\",\n",
    "    \"PCTENRHS\",\n",
    "    \"PCTENRAP\",\n",
    "    \"PCTENRAN\",\n",
    "    \"PCTENRNR\",\n",
    "    \"PCTENRUN\"\n",
    "]\n",
    "\n",
    "with open(DATA_TYPES_PATH) as f:\n",
    "    cur_datatypes = json.load(f)\n",
    "\n",
    "for per_col in percent_columns:\n",
    "    df = df.withColumn(per_col, F.col(per_col).cast(\"float\"))\n",
    "    df = df.withColumn(per_col, col(per_col) / 100)\n",
    "    cur_datatypes[per_col] = \"float\"\n",
    "\n",
    "with open(DATA_TYPES_PATH, 'w') as f:\n",
    "    f.write(json.dumps(cur_datatypes, indent=4))\n",
    "\n",
    "cur_columns = df.columns\n",
    "\n",
    "# add new columns for override\n",
    "hd2020_df = spark.read.csv(\"./data/v2_additional_data/HD2020.csv\", header=True, inferSchema=True)\n",
    "drvef2020_df = spark.read.csv(\"./data/v2_additional_data/DRVEF2020.csv\", header=True, inferSchema=True)\n",
    "\n",
    "override_nonexist_columns = {\n",
    "    \"WEBADDR\": \"string\",\n",
    "    \"NPRICURL\": \"string\",\n",
    "    \"PCTENR2M\": \"float\"\n",
    "}\n",
    "\n",
    "add_datatypes(override_nonexist_columns)\n",
    "\n",
    "override_hd = hd2020_df.select(\"UNITID\", \"WEBADDR\", \"NPRICURL\")\n",
    "override_hd = override_hd.withColumn(\"UNITID\", F.col(\"UNITID\").cast(\"string\"))\n",
    "override_drvef = drvef2020_df.select(\"UNITID\", \"PCTENR2M\")\n",
    "override_drvef = override_drvef.withColumn(\"UNITID\", F.col(\"UNITID\").cast(\"string\"))\n",
    "\n",
    "df = df.join(override_hd, \"UNITID\", \"left\")\n",
    "df = df.join(override_drvef, \"UNITID\", \"left\")\n",
    "df = df.withColumn(\"PCTENR2M\", F.col(\"PCTENR2M\").cast(\"float\"))\n",
    "df = df.withColumn(\"PCTENR2M\", col(\"PCTENR2M\") / 100)\n",
    "df = df.replace({'NULL': None, 'null': None})\n",
    "null_counts = df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Save dictionary of null value counts for reference\n",
    "with open('./assets/null_counts.json', 'w') as f:\n",
    "    json.dump(null_counts, f)\n",
    "def column_override(first, second):\n",
    "    if first is None:\n",
    "        return second\n",
    "    return first\n",
    "\n",
    "udf_1 = udf(column_override, StringType())\n",
    "udf_2 = udf(column_override, FloatType())\n",
    "df = df.withColumn(\"INSTURL\", udf_1(\"WEBADDR\", \"INSTURL\"))\n",
    "df = df.withColumn(\"NPCURL\", udf_1(\"NPRICURL\", \"NPCURL\"))\n",
    "df = df.withColumn(\"UGDS_2MOR\", udf_2(\"PCTENR2M\", \"UGDS_2MOR\"))\n",
    "df = df.withColumn(\"UGDS_WHITE\", udf_2(\"PCTENRWH\", \"UGDS_WHITE\"))\n",
    "df = df.withColumn(\"UGDS_BLACK\", udf_2(\"PCTENRBK\", \"UGDS_BLACK\"))\n",
    "df = df.withColumn(\"UGDS_AIAN\", udf_2(\"PCTENRAN\", \"UGDS_AIAN\"))\n",
    "df = df.withColumn(\"UGDS_NRA\", udf_2(\"PCTENRNR\", \"UGDS_NRA\"))\n",
    "df = df.withColumn(\"UGDS_UNKN\", udf_2(\"PCTENRUN\", \"UGDS_UNKN\"))\n",
    "\n",
    "# check whether override is success\n",
    "null_counts_2 = df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Save dictionary of null value counts for reference\n",
    "with open('./assets/null_counts_v3.json', 'w') as f:\n",
    "    json.dump(null_counts_2, f)\n",
    "\n",
    "# use PySpark to check and convert data types\n",
    "with open('./assets/datatypes.json') as f:\n",
    "    datatypes = json.load(f)\n",
    "\n",
    "# cast columns to correct datatypes\n",
    "for field, datatype in datatypes.items(): # takes a few mins to run\n",
    "    if field not in df.columns:\n",
    "        continue\n",
    "    curr_type = dict(df.dtypes)[field]\n",
    "    if curr_type != datatype and not datatype.startswith(curr_type):\n",
    "        df = df.withColumn(field, F.col(field).cast(datatype))\n",
    "\n",
    "# Wrap up\n",
    "# Modified by Rithwick: Add \"overwrite\" mode to make the program run without removing the V3 Parquet directory\n",
    "# df.write.mode('overwrite').save(V3_PATH)\n",
    "#df.write.save(V3_PATH)\n",
    "\n",
    "# Modified by Rithwick: Fixed bug by moving to the end because the latter part of the program is using Spark context\n",
    "# sc.stop()\n",
    "\n",
    "non_override_columns = {\n",
    "    \"CIP01BACHL\": \"integer\",\n",
    "    \"CIP03BACHL\": \"integer\",\n",
    "    \"CIP04BACHL\": \"integer\",\n",
    "    \"CIP05BACHL\": \"integer\",\n",
    "    \"CIP09BACHL\": \"integer\",\n",
    "    \"CIP10BACHL\": \"integer\",\n",
    "    \"CIP11BACHL\": \"integer\",\n",
    "    \"CIP12BACHL\": \"integer\",\n",
    "    \"CIP13BACHL\": \"integer\",\n",
    "    \"CIP14BACHL\": \"integer\",\n",
    "    \"CIP15BACHL\": \"integer\",\n",
    "    \"CIP16BACHL\": \"integer\",\n",
    "    \"CIP19BACHL\": \"integer\",\n",
    "    \"CIP22BACHL\": \"integer\",\n",
    "    \"CIP23BACHL\": \"integer\",\n",
    "    \"CIP24BACHL\": \"integer\",\n",
    "    \"CIP25BACHL\": \"integer\",\n",
    "    \"CIP26BACHL\": \"integer\",\n",
    "    \"CIP27BACHL\": \"integer\",\n",
    "    \"CIP29BACHL\": \"integer\",\n",
    "    \"CIP30BACHL\": \"integer\",\n",
    "    \"CIP31BACHL\": \"integer\",\n",
    "    \"CIP38BACHL\": \"integer\",\n",
    "    \"CIP39BACHL\": \"integer\",\n",
    "    \"CIP40BACHL\": \"integer\",\n",
    "    \"CIP41BACHL\": \"integer\",\n",
    "    \"CIP42BACHL\": \"integer\",\n",
    "    \"CIP43BACHL\": \"integer\",\n",
    "    \"CIP44BACHL\": \"integer\",\n",
    "    \"CIP45BACHL\": \"integer\",\n",
    "    \"CIP46BACHL\": \"integer\",\n",
    "    \"CIP47BACHL\": \"integer\",\n",
    "    \"CIP48BACHL\": \"integer\",\n",
    "    \"CIP49BACHL\": \"integer\",\n",
    "    \"CIP50BACHL\": \"integer\",\n",
    "    \"CIP51BACHL\": \"integer\",\n",
    "    \"CIP52BACHL\": \"integer\",\n",
    "    \"CIP54BACHL\": \"integer\",\n",
    "    \"NPT41_PUB\": \"integer\",\n",
    "    \"NPT42_PUB\": \"integer\",\n",
    "    \"NPT43_PUB\": \"integer\",\n",
    "    \"NPT44_PUB\": \"integer\",\n",
    "    \"NPT45_PUB\": \"integer\",\n",
    "    \"NPT41_PRIV\": \"integer\",\n",
    "    \"NPT42_PRIV\": \"integer\",\n",
    "    \"NPT43_PRIV\": \"integer\",\n",
    "    \"NPT44_PRIV\": \"integer\",\n",
    "    \"NPT45_PRIV\": \"integer\",\n",
    "    \"GRAD_DEBT_MDN\": \"float\",\n",
    "    \"LO_INC_DEBT_MDN\": \"float\",\n",
    "    \"MD_INC_DEBT_MDN\": \"float\",\n",
    "    \"HI_INC_DEBT_MDN\": \"float\",\n",
    "    \"FIRSTGEN_DEBT_MDN\": \"float\",\n",
    "    \"NOTFIRSTGEN_DEBT_MDN\": \"float\",\n",
    "    \"FIRST_GEN\": \"float\",\n",
    "    \"UNEMP_RATE\": \"float\",\n",
    "    \"MN_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT10_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT25_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT75_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT90_EARN_WNE_P10\": \"integer\",\n",
    "    \"PCT10_EARN_WNE_P6\": \"integer\",\n",
    "    \"PCT25_EARN_WNE_P6\": \"integer\",\n",
    "    \"PCT75_EARN_WNE_P6\": \"integer\",\n",
    "    \"PCT90_EARN_WNE_P6\": \"integer\",\n",
    "}\n",
    "\n",
    "# add datatypes\n",
    "add_datatypes(non_override_columns)\n",
    "scorecard_df = spark.read.csv(SCORECARD_PATH, header=True, inferSchema=True)\n",
    "print((scorecard_df.count(), len(scorecard_df.columns)))\n",
    "\n",
    "non_override_colnames = [\"UNITID\"]\n",
    "for colname, datatype in non_override_columns.items():\n",
    "    non_override_colnames.append(colname)\n",
    "non_override_df = scorecard_df.select(*non_override_colnames)\n",
    "non_override_df = non_override_df.withColumn(\"UNITID\", F.col(\"UNITID\").cast(\"string\"))\n",
    "print(\"Non-override: count = {0}, columns = {1}\". format(non_override_df.count(), len(non_override_df.columns)))\n",
    "\n",
    "df = df.join(non_override_df, \"UNITID\", \"left\")\n",
    "col_set = set(df.columns)\n",
    "print(\"Final Dataset (joined with UNITID), count = {0}, len of columns = {1}, # of columns = {2}\". format(df.count(), len(col_set), len(df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up: save file, end spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
